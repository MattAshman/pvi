{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "massive-jonathan",
   "metadata": {},
   "source": [
    "# Continual learning sparse Gaussian processes \n",
    "\n",
    "Reproducing the results of [Bui et al.](https://arxiv.org/pdf/1811.11206.pdf) on the Banana classification dataset.\n",
    "\n",
    "* Compares the results using maximum likelhood learning of hyperparameters to Bayesian treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../../\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pvi.models.sgp import SparseGaussianProcessClassification\n",
    "from pvi.distributions import MultivariateGaussianDistributionWithZ, MultivariateGaussianFactorWithZ\n",
    "from pvi.distributions import GammaDistribution, GammaFactor, LogNormalDistribution, LogNormalFactor\n",
    "from pvi.distributions.hypers import HyperparameterDistribution, HyperparameterFactor\n",
    "from pvi.clients import ContinualLearningSGPClient, ContinualLearningSGPClientBayesianHypers\n",
    "from pvi.servers import ContinualLearningServer, ContinualLearningServerBayesianHypers\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.auto as tqdm\n",
    "import gpytorch\n",
    "\n",
    "from torch import nn\n",
    "from pvi.models.kernels import RBFKernel\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-sheet",
   "metadata": {},
   "source": [
    "### Set up data and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-specific",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(name, train_proportion, data_base_dir):\n",
    "    filename = os.path.join(data_base_dir, name, \"banana.csv\")\n",
    "    data = np.loadtxt(filename, delimiter=\",\", skiprows=1)\n",
    "    \n",
    "    x = data[:, :2]\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    # Replace 1's with 0's and 2's with 1's.\n",
    "    y[y == 1] = 0\n",
    "    y[y == 2] = 1\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    N_train = int(np.ceil(train_proportion * N))\n",
    "    \n",
    "    x_train = x[0:N_train]\n",
    "    y_train = y[0:N_train]\n",
    "    x_test = x[N_train:]\n",
    "    y_test = y[N_train:]\n",
    "\n",
    "    training_set = {\n",
    "        \"x\": x_train,\n",
    "        \"y\": y_train,\n",
    "    }\n",
    "\n",
    "    test_set = {\n",
    "        \"x\": x_test,\n",
    "        \"y\": y_test\n",
    "    }\n",
    "\n",
    "    D = x_test.shape[1]\n",
    "\n",
    "    return training_set, test_set, D\n",
    "\n",
    "def generate_clients_data(x, y, M, dataset_seed):\n",
    "        # this function ought to return a list of (x, y) tuples.\n",
    "        # you need to set the seed in the main experiment file to ensure that this function becomes deterministic\n",
    "\n",
    "        random_state = np.random.get_state()\n",
    "\n",
    "        if dataset_seed is not None:\n",
    "            np.random.seed(dataset_seed)\n",
    "\n",
    "        if M == 1:\n",
    "            client_data = [{\"x\": x, \"y\": y}]\n",
    "            N_is = [x.shape[0]]\n",
    "            props_positive = [np.mean(y > 0)]\n",
    "\n",
    "            return client_data, N_is, props_positive, M\n",
    "\n",
    "        N = x.shape[0]\n",
    "        client_size = int(np.floor(N/M))\n",
    "\n",
    "        class_balance = np.mean(y == 0)\n",
    "\n",
    "        pos_inds = np.where(y > 0)\n",
    "        zero_inds = np.where(y == 0)\n",
    "        \n",
    "        assert (len(pos_inds[0]) + len(zero_inds[0])) == len(y), \"Some indeces missed.\"\n",
    "        \n",
    "        print(f'x shape {x.shape}')\n",
    "#         print(f'positive indices {pos_inds}')\n",
    "#         print(f'zero indices {zero_inds}')\n",
    "\n",
    "        y_pos = y[pos_inds]\n",
    "        y_neg = y[zero_inds]\n",
    "\n",
    "        x_pos = x[pos_inds]\n",
    "        x_neg = x[zero_inds]\n",
    "\n",
    "        client_data = []\n",
    "\n",
    "        # Recombine remaining data and shuffle.\n",
    "        x = np.concatenate([x_pos, x_neg])\n",
    "        y = np.concatenate([y_pos, y_neg])\n",
    "        \n",
    "        # As in Bui et al, order according to x1 value.\n",
    "        inds = np.argsort(x[:, 0])\n",
    "\n",
    "        x = x[inds]\n",
    "        y = y[inds]\n",
    "\n",
    "        # Distribute among large clients.\n",
    "        for i in range(M):\n",
    "            client_x = x[:client_size]\n",
    "            client_y = y[:client_size]\n",
    "\n",
    "            x = x[client_size:]\n",
    "            y = y[client_size:]\n",
    "\n",
    "            client_data.append({'x': client_x, 'y': client_y})\n",
    "\n",
    "        N_is = [data['x'].shape[0] for data in client_data]\n",
    "        props_positive = [np.mean(data['y'] > 0) for data in client_data]\n",
    "\n",
    "        np.random.set_state(random_state)\n",
    "\n",
    "        print(f'N_is {N_is}')\n",
    "        print(f'Props positive: {props_positive}')\n",
    "\n",
    "        return client_data, N_is, props_positive, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-highway",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data(x, y, ax=None):\n",
    "    x1_min, x1_max = -3., 3.\n",
    "    x2_min, x2_max = -3., 3.\n",
    "    \n",
    "    x1x1, x2x2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), \n",
    "                             np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(8, 6), dpi=200)\n",
    "        plt.xlim(x1x1.min(), x1x1.max())\n",
    "        plt.ylim(x2x2.min(), x2x2.max())\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    ax.plot(x[y == 0, 0], x[y == 0, 1], \"o\", color=\"C1\", label=\"Class 1\", alpha=.5)\n",
    "    ax.plot(x[y == 1, 0], x[y == 1, 1], \"o\", color=\"C0\", label=\"Class 2\", alpha=.5)\n",
    "    \n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.legend(loc=\"upper left\", scatterpoints=1, numpoints=1)\n",
    "    \n",
    "    return x1x1, x2x2\n",
    "\n",
    "def plot_predictive_distribution(x, y, z, model, q, ax=None, x_old=None, y_old=None, z_old=None):\n",
    "    x1x1, x2x2 = plot_data(x, y, ax)\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    x_predict = np.concatenate((x1x1.ravel().reshape(-1, 1), \n",
    "                                x2x2.ravel().reshape(-1, 1)), 1)\n",
    "    \n",
    "    y_predict = model(torch.tensor(x_predict), q=q, diag=True)\n",
    "    y_predict = y_predict.mean.detach().numpy().reshape(x1x1.shape)\n",
    "    \n",
    "    cs2 = ax.contour(x1x1, x2x2, y_predict, colors=[\"k\"], lw=2, levels=[0.2, 0.5, 0.8])\n",
    "    ax.clabel(cs2, fmt=\"%2.1f\", colors=\"k\", fontsize=14)\n",
    "        \n",
    "    ax.scatter(z[:, 0], z[:, 1], color=\"r\", marker=\"s\")\n",
    "    \n",
    "    if x_old is not None:\n",
    "        ax.plot(x_old[y_old == 0, 0], x_old[y_old == 0, 1], \"o\", color=\"C1\", label=\"Class 1\", alpha=.1)\n",
    "        ax.plot(x_old[y_old == 1, 0], x_old[y_old == 1, 1], \"o\", color=\"C0\", label=\"Class 2\", alpha=.1)\n",
    "        ax.scatter(z_old[:, 0], z_old[:, 1], color=\"r\", marker=\"o\")        \n",
    "        \n",
    "def plot_bayesian_predictive_distribution(x, y, z, model, q, qeps, ax=None, x_old=None, y_old=None, z_old=None, neps=10):\n",
    "    x1x1, x2x2 = plot_data(x, y, ax)\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    x_predict = np.concatenate((x1x1.ravel().reshape(-1, 1), \n",
    "                                x2x2.ravel().reshape(-1, 1)), 1)\n",
    "    \n",
    "    preds = []\n",
    "    for _ in range(neps):\n",
    "        eps = qeps.sample()\n",
    "        model.hyperparameters = eps\n",
    "        pp = model(torch.tensor(x_predict), q, diag=True)\n",
    "        preds.append(pp.mean.detach().numpy())\n",
    "        \n",
    "    y_predict = (sum(preds) / len(preds)).reshape(x1x1.shape)\n",
    "    \n",
    "    cs2 = ax.contour(x1x1, x2x2, y_predict, colors=[\"k\"], lw=2, levels=[0.2, 0.5, 0.8])\n",
    "    ax.clabel(cs2, fmt=\"%2.1f\", colors=\"k\", fontsize=14)\n",
    "        \n",
    "    ax.scatter(z[:, 0], z[:, 1], color=\"r\", marker=\"s\")\n",
    "    \n",
    "    if x_old is not None:\n",
    "        ax.plot(x_old[y_old == 0, 0], x_old[y_old == 0, 1], \"o\", color=\"C1\", label=\"Class 1\", alpha=.1)\n",
    "        ax.plot(x_old[y_old == 1, 0], x_old[y_old == 1, 1], \"o\", color=\"C0\", label=\"Class 2\", alpha=.1)\n",
    "        ax.scatter(z_old[:, 0], z_old[:, 1], color=\"r\", marker=\"o\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"banana\"\n",
    "data_base_dir = \"/Users/matt/projects/pvi/datasets\"\n",
    "train_proportion = 0.08\n",
    "\n",
    "training_set, test_set, D = load_data(\n",
    "    name, train_proportion, data_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "\n",
    "clients_data, nis, prop_positive, M = generate_clients_data(\n",
    "    training_set[\"x\"], \n",
    "    training_set[\"y\"],\n",
    "    M=M,\n",
    "    dataset_seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-tuner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data(training_set[\"x\"], training_set[\"y\"])\n",
    "plt.grid(b=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-polls",
   "metadata": {},
   "source": [
    "# ML training of hyperparameters\n",
    "\n",
    "Hyperparameters are optimised by maximising the ELBO at each dataset, defined by\n",
    "\n",
    "$$\n",
    "\\mathcal{F}(q_{\\text{new}}(f), \\theta_{\\text{new}}) = \\mathrm{KL}\\left(q(\\mathbf{b}, \\mathbf{a}) \\| p\\left(\\mathbf{b}, \\mathbf{a} | \\theta_{\\mathrm{new}}\\right)\\right)-\\int \\mathrm{d} f q_{\\mathrm{new}}(f) \\log p\\left(\\mathbf{y}_{\\text {new }} | f\\right) - \\int \\mathrm{d}\\mathbf{a} q(\\mathbf{a}) \\log \\frac{q(\\mathbf{a})}{p\\left(\\mathbf{a} | \\theta_{\\mathrm{old}}\\right)}.\n",
    "$$\n",
    "\n",
    "where $$q_{old}(f) = p(f_{\\setminus \\mathbf{a}} | \\mathbf{a})q(\\mathbf{a})$$ is the old approximate posterior and $$q_{new}(f) = p(f_{\\setminus \\mathbf{a}\\mathbf{b}} | \\mathbf{a}, \\mathbf{b}, \\theta_{new})q(\\mathbf{a})q(\\mathbf{b} | \\mathbf{a})$$ is the new approximate posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inducing = 10\n",
    "\n",
    "server_config = {\n",
    "    \"max_iterations\": M,\n",
    "    \"train_model\": True,\n",
    "    \"model_optimiser_params\": {\"lr\": 5e-2},\n",
    "    \"num_eps_samples\": 10,\n",
    "}\n",
    "\n",
    "init_nat_params = {\n",
    "    \"np1\": torch.zeros(num_inducing),\n",
    "    \"np2\": torch.zeros(num_inducing).diag_embed(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"D\": D,\n",
    "    \"num_inducing\": num_inducing,\n",
    "    \"kernel_class\": lambda **kwargs: RBFKernel(**kwargs),\n",
    "    \"kernel_params\": {\n",
    "        \"ard_num_dims\": D, \n",
    "        \"train_hypers\": True\n",
    "    },\n",
    "    \"num_predictive_samples\": 100\n",
    "}\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"outputscale\": 2.,\n",
    "    \"lengthscale\": torch.ones(D) * 1.\n",
    "}\n",
    "\n",
    "client_config = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"optimiser_params\": {\"lr\": 1e-2},\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": len(clients_data[0][\"x\"]),\n",
    "    \"num_elbo_samples\": 100,\n",
    "    \"num_elbo_hyper_samples\": 2,\n",
    "    \"num_predictive_samples\": 100,\n",
    "    \"train_model\": True,\n",
    "}\n",
    "\n",
    "ml_clients = []\n",
    "# Construct clients.\n",
    "for i in range(M):\n",
    "    model_i = SparseGaussianProcessClassification(config=model_config, hyperparameters=model_hyperparameters)\n",
    "    data_i = clients_data[i]\n",
    "    \n",
    "    # Convert to torch.tensor.\n",
    "    for k, v in data_i.items():\n",
    "        data_i[k] = torch.tensor(v)\n",
    "    \n",
    "    # Randomly initialise private inducing points.\n",
    "    perm = torch.randperm(len(data_i[\"x\"]))\n",
    "    idx = perm[:model_config[\"num_inducing\"]]\n",
    "    z_i = torch.tensor(data_i[\"x\"][idx]).clone()\n",
    "        \n",
    "    ml_clients.append(ContinualLearningSGPClient(data=data_i, model=model_i, inducing_locations=z_i, config=client_config))\n",
    "\n",
    "# Construct global model and server.\n",
    "model = SparseGaussianProcessClassification(config=model_config, hyperparameters=model_hyperparameters)\n",
    "\n",
    "q = MultivariateGaussianDistributionWithZ(\n",
    "    inducing_locations=None,\n",
    "    nat_params=None, \n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "ml_server = ContinualLearningServer(\n",
    "    model=model, \n",
    "    q=q, \n",
    "    clients=ml_clients,\n",
    "    config=server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not ml_server.should_stop():\n",
    "    ml_server.tick()\n",
    "\n",
    "    # Obtain predictions.\n",
    "    pp = ml_server.model_predict(torch.tensor(test_set[\"x\"]), diag=True)\n",
    "    preds = pp.mean.detach().numpy()\n",
    "    test_acc = np.sum(\n",
    "        np.abs(2 * (preds > 0.5) - 1 + test_set[\"y\"]) > 0) / np.size(test_set[\"y\"])\n",
    "\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=3, dpi=200)\n",
    "\n",
    "x_old, y_old, z_old = None, None, None\n",
    "for q, model_state_dict, ax, client in zip(ml_server.log[\"q\"][1:], ml_server.log[\"model_state_dict\"][1:], axes, ml_clients):\n",
    "    x, y = client.data[\"x\"].numpy(), client.data[\"y\"].numpy()\n",
    "    z = client.inducing_locations.numpy()\n",
    "    \n",
    "    model = copy.deepcopy(ml_server.model)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    plot_predictive_distribution(x, y, z, model, q=q, ax=ax, x_old=x_old, y_old=y_old, z_old=z_old)\n",
    "    \n",
    "    if x_old is not None:\n",
    "        x_old = np.concatenate((x_old, x), axis=0)\n",
    "        y_old = np.concatenate((y_old, y), axis=0)\n",
    "        z_old = np.concatenate((z_old, z), axis=0)\n",
    "    else:\n",
    "        x_old, y_old, z_old = x, y, z\n",
    "    \n",
    "plt.suptitle(\"f: vfe, hypers: ml\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-canal",
   "metadata": {},
   "source": [
    "# Bayesian treatment of hyperparameters\n",
    "\n",
    "Old approximate posterior: $$q_{old}(f, \\theta) = p(f_{\\setminus \\mathbf{a}} | \\mathbf{a}, \\theta) q(\\mathbf{a}) q_{old}(\\theta).$$\n",
    "New approximate posterior: $$q_{new}(f, \\theta) = p(f_{\\setminus \\mathbf{a}, \\mathbf{b}} | \\mathbf{a}, \\mathbf{b}, \\theta) q(\\mathbf{a}) q(\\mathbf{b} | \\mathbf{a}) q_{new}(\\theta).$$\n",
    "\n",
    "The variational objective is given by\n",
    "$$\\mathcal{F}(q_{new}(f, \\theta)) = \\mathrm{KL}\\left[q_{\\mathrm{new}}(\\theta) \\| q_{\\mathrm{old}}(\\theta)\\right]+\\int \\mathrm{d} \\theta q(\\theta)(\\mathrm{KL}[q(\\mathbf{a}, \\mathbf{b}) \\| p(\\mathbf{a}, \\mathbf{b} \\mid \\theta)]) -\\int \\mathrm{d} \\theta q(\\theta)(\\mathrm{KL}[q(\\mathbf{a}) \\| p(\\mathbf{a} \\mid \\theta)])-\\int \\mathrm{d} f \\mathrm{~d} \\theta q_{\\text {new }}(f, \\theta) \\log p\\left(\\mathbf{y}_{\\text {new }} \\mid f, \\theta\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_log_lengthscale_std_params = {\n",
    "    \"loc\": torch.zeros(D),\n",
    "    \"scale\": torch.ones(D) * .5,\n",
    "}\n",
    "\n",
    "prior_log_outputscale_std_params = {\n",
    "    \"loc\": torch.tensor(1.),\n",
    "    \"scale\": torch.tensor(1.),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"D\": D,\n",
    "    \"num_inducing\": num_inducing,\n",
    "    \"kernel_class\": lambda **kwargs: RBFKernel(**kwargs),\n",
    "    \"kernel_params\": {\n",
    "        \"ard_num_dims\": D, \n",
    "        \"train_hypers\": False\n",
    "    },\n",
    "    \"num_predictive_samples\": 100\n",
    "}\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"outputscale\": 1.,\n",
    "    \"lengthscale\": torch.ones(D) * 1.,\n",
    "}\n",
    "\n",
    "client_config = {\n",
    "    \"optimiser\": \"Adam\",\n",
    "    \"optimiser_params\": {\"lr\": 1e-2},\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": len(clients_data[0][\"x\"]),\n",
    "    \"num_elbo_samples\": 10,\n",
    "    \"num_elbo_hyper_samples\": 1,\n",
    "    \"num_predictive_samples\": 100,\n",
    "    \"train_model\": True,\n",
    "}\n",
    "\n",
    "bayes_clients = []\n",
    "# Construct clients.\n",
    "for i in range(M):\n",
    "    model_i = SparseGaussianProcessClassification(config=model_config, hyperparameters=model_hyperparameters)\n",
    "    data_i = clients_data[i]\n",
    "    \n",
    "    # Convert to torch.tensor.\n",
    "    for k, v in data_i.items():\n",
    "        data_i[k] = torch.tensor(v)\n",
    "        \n",
    "    # Randomly initialise private inducing points.\n",
    "    perm = torch.randperm(len(data_i[\"x\"]))\n",
    "    idx = perm[:model_config[\"num_inducing\"]]\n",
    "    z_i = torch.tensor(data_i[\"x\"][idx]).clone()\n",
    "    \n",
    "    bayes_clients.append(ContinualLearningSGPClientBayesianHypers(\n",
    "        data=data_i, model=model_i, inducing_locations=z_i, config=client_config))\n",
    "\n",
    "# Construct global model and server.\n",
    "model = SparseGaussianProcessClassification(config=model_config)\n",
    "\n",
    "q = MultivariateGaussianDistributionWithZ(\n",
    "    inducing_locations=None,\n",
    "    nat_params=None, \n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "qeps = HyperparameterDistribution(\n",
    "    {\n",
    "        \"lengthscale\": LogNormalDistribution(std_params=prior_log_lengthscale_std_params, is_trainable=True),\n",
    "        \"outputscale\": LogNormalDistribution(std_params=prior_log_outputscale_std_params, is_trainable=True)\n",
    "    }\n",
    ")\n",
    "\n",
    "bayesian_server = ContinualLearningServerBayesianHypers(\n",
    "    model=model, \n",
    "    q=q, \n",
    "    qeps=qeps,\n",
    "    clients=bayes_clients,\n",
    "    config=server_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not bayesian_server.should_stop():\n",
    "    bayesian_server.tick()\n",
    "\n",
    "    # Obtain predictions.\n",
    "    dists = bayesian_server.model_predict(torch.tensor(test_set[\"x\"]), diag=True)\n",
    "\n",
    "    preds = []\n",
    "    for pp in dists:\n",
    "        preds.append(pp.mean.detach().numpy())\n",
    "\n",
    "    preds = np.array(preds).mean(0)\n",
    "    test_acc = np.sum(\n",
    "        np.abs(2 * (preds > 0.5) - 1 + test_set[\"y\"]) > 0) / np.size(test_set[\"y\"])\n",
    "\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = training_set[\"x\"][::5], training_set[\"y\"][::5]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=3, dpi=200)\n",
    "\n",
    "x_old, y_old, z_old = None, None, None\n",
    "for q, qeps, ax, client in zip(bayesian_server.log[\"q\"][1:], bayesian_server.log[\"qeps\"][1:], axes, bayesian_server.clients):\n",
    "    x, y = client.data[\"x\"].numpy(), client.data[\"y\"].numpy()\n",
    "    z = client.inducing_locations.numpy()\n",
    "    \n",
    "    plot_bayesian_predictive_distribution(x, y, z, bayesian_server.model, q=q, qeps=qeps, ax=ax, x_old=x_old, y_old=y_old, z_old=z_old)\n",
    "    \n",
    "    if x_old is not None:\n",
    "        x_old = np.concatenate((x_old, x), axis=0)\n",
    "        y_old = np.concatenate((y_old, y), axis=0)\n",
    "        z_old = np.concatenate((z_old, z), axis=0)\n",
    "    else:\n",
    "        x_old, y_old, z_old = x, y, z\n",
    "    \n",
    "plt.suptitle(\"f: vfe, hypers: vfe\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-diversity",
   "metadata": {},
   "source": [
    "# Visualise hyperparameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 9), nrows=3, ncols=3, dpi=200)\n",
    "\n",
    "for i, (ml_model_state_dict, qeps, row) in enumerate(zip(ml_server.log[\"model_state_dict\"][1:], bayesian_server.log[\"qeps\"][1:], axes)):\n",
    "    \n",
    "    ml_model = copy.deepcopy(ml_server.model)\n",
    "    ml_model.load_state_dict(ml_model_state_dict)\n",
    "        \n",
    "    lx1 = torch.logspace(-.5, .5, 1000).unsqueeze(-1)\n",
    "    lx2 = torch.logspace(-.5, .5, 1000).unsqueeze(-1)\n",
    "    lx = torch.cat((lx1, lx2), dim=1)\n",
    "    osx = torch.logspace(0, 2.5, 1000)\n",
    "\n",
    "    q_l = qeps.distributions[\"lengthscale\"].distribution\n",
    "    q_os = qeps.distributions[\"outputscale\"].distribution\n",
    "    \n",
    "    q_l_probs = q_l.log_prob(lx).exp()\n",
    "    q_os_probs = q_os.log_prob(osx).exp()\n",
    "\n",
    "    row[0].plot(lx1.log(), q_l_probs[:, 0], c=\"C1\", alpha=1.)\n",
    "    row[0].axvline(ml_model.kernel.lengthscale[0].log().item(), c=\"k\")\n",
    "    \n",
    "    row[1].plot(lx2.log(), q_l_probs[:, 1], c=\"C1\", alpha=1., label=\"Dataset {}\".format(i))\n",
    "    row[1].axvline(ml_model.kernel.lengthscale[1].log().item(), c=\"k\")\n",
    "\n",
    "    row[2].plot(osx.log(), q_os_probs, c=\"C1\", alpha=1.)\n",
    "    row[2].axvline(ml_model.kernel.outputscale.log().item(), c=\"k\")\n",
    "    \n",
    "axes[0, 0].set_title(\"log lengthscale 0\")\n",
    "axes[0, 1].set_title(\"log lengthscale 1\")\n",
    "axes[0, 2].set_title(\"log sigma\")\n",
    "axes[0, 0].set_ylabel(\"density\")\n",
    "axes[1, 0].set_ylabel(\"density\")\n",
    "axes[2, 0].set_ylabel(\"density\")\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.grid(b=True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-meaning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
